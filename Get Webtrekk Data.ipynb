{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c32439d2-49d1-4189-b62c-4b5a6ddc5cbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "folder_path = \"/Volumes/swi_audience_prd/swi_posts/payloads/\"\n",
    "files = [f.path for f in dbutils.fs.ls(folder_path) if f.path.endswith(\".txt\")]\n",
    "\n",
    "payloads = []\n",
    "for path in files:\n",
    "    lines_df = spark.read.text(path)\n",
    "    raw = \"\\n\".join(r.value for r in lines_df.collect())\n",
    "    # Ersetze \"upperLimit\": <zahl> durch \"upperLimit\": 5000000\n",
    "    raw = re.sub(r'(\"upperLimit\"\\s*:\\s*)\\d+', r'\\g<1>5000000', raw)\n",
    "    payload = json.loads(raw)\n",
    "    payloads.append(payload)\n",
    "\n",
    "# (Optional) Sicher ausgeben\n",
    "for payload in payloads:\n",
    "    print(json.dumps(payload, ensure_ascii=False, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "40981b05-5274-46a8-9b5d-148412df8ecd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_names = [\n",
    "    \"swi_audience_prd.swi_posts.mapp_api_query_\" + f.replace(\"dbfs:/Volumes/swi_audience_prd/swi_posts/payloads/\", \"\")\n",
    "                        .replace(\".txt\", \"\")\n",
    "                        .replace(\".\", \"_\")\n",
    "    for f in files\n",
    "]\n",
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "403594de-98b3-4081-9d27-1f089486723e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import json\n",
    "import time  # Add this import\n",
    "\n",
    "# Zugangsdaten (SECRET_SCOPE = \"swi-secret-scope\"\n",
    "# F√ºr Hilfe: Keller, Pascal (SRF) oder hier: https://github.com/mmz-srf/swi-analytics-databricks/blob/main/intelligence.eu.mapp.com_analysis-query_7455/Secret%20management%20in%20databricks.ipynb)\n",
    "SECRET_SCOPE = \"swi-secret-scope\"\n",
    "user   = dbutils.secrets.get(SECRET_SCOPE, \"mapp-user\")\n",
    "secret = dbutils.secrets.get(SECRET_SCOPE, \"mapp-secret\")\n",
    "try:\n",
    "    baseurl = dbutils.secrets.get(SECRET_SCOPE, \"mapp-baseurl\")\n",
    "except:\n",
    "    baseurl = \"https://intelligence.eu.mapp.com\"\n",
    "\n",
    "token_file = 'mapp_token.json'\n",
    "\n",
    "# Pr√ºfen, ob bereits ein Token existiert und ob es noch g√ºltig ist\n",
    "def get_token():\n",
    "    # Pr√ºfen, ob bereits ein Token existiert und ob es noch g√ºltig ist\n",
    "    if os.path.exists(token_file):\n",
    "        with open(token_file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            token = data.get('access_token')\n",
    "            expires_at = data.get('expires_at')\n",
    "            if token and expires_at and time.time() < expires_at:\n",
    "                return token  # ‚è≥ Noch g√ºltig\n",
    "\n",
    "    # üÜï Token holen\n",
    "    auth_url = f\"{baseurl}/analytics/api/oauth/token\"\n",
    "    querystring = {\"grant_type\": \"client_credentials\", \"scope\": \"mapp.intelligence-api\"}\n",
    "    response = requests.post(auth_url, auth=(user, secret), params=querystring)\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    token = result['access_token']\n",
    "    expires_in = result.get('expires_in', 3600)  # meist 3600 Sekunden\n",
    "    expires_at = time.time() + expires_in - 60   # etwas Puffer\n",
    "\n",
    "    # Token speichern f√ºr sp√§ter\n",
    "    with open(token_file, 'w') as f:\n",
    "        json.dump({'access_token': token, 'expires_at': expires_at}, f)\n",
    "\n",
    "    return token\n",
    "\n",
    "# Token abrufen\n",
    "token = get_token()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "122d078c-040f-44f5-912c-2385e14bf452",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for i, payload in enumerate(payloads):\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    url = f\"{baseurl}/analytics/api/analysis-query\"\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    result = response.json()\n",
    "\n",
    "    resultUrl = result.get(\"resultUrl\")\n",
    "    statusUrl = result.get(\"statusUrl\")\n",
    "\n",
    "    tries = 0\n",
    "    while not resultUrl and tries < 10:\n",
    "        time.sleep(10)\n",
    "        status_response = requests.get(statusUrl, headers=headers)\n",
    "        result = status_response.json()\n",
    "        resultUrl = result.get(\"resultUrl\")\n",
    "        tries += 1\n",
    "\n",
    "    if not resultUrl:\n",
    "        print(f\"‚ùå Kein Ergebnis f√ºr Payload {i} nach mehreren Versuchen.\")\n",
    "        continue\n",
    "    else:\n",
    "        result_data = requests.get(resultUrl, headers=headers).json()\n",
    "\n",
    "        headers_out = [col[\"name\"] for col in result_data[\"headers\"]]\n",
    "        rows = result_data[\"rows\"]\n",
    "\n",
    "        df = spark.createDataFrame(rows, headers_out)\n",
    "\n",
    "        row_count = df.count()\n",
    "        if row_count > 1:\n",
    "            df = df.limit(row_count - 1)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Zu wenige Zeilen zum K√ºrzen f√ºr Payload {i}.\")\n",
    "\n",
    "        display(df)\n",
    "        # ‚úÖ Speichern als Delta Table (Version 2.0)\n",
    "        # (Dynamischer Name aus der Liste `table_names`)\n",
    "        df.write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .saveAsTable(table_names[i])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Get Webtrekk Data",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
